import sys, os, gzip, argparse, logging, warnings, shutil, subprocess, ast, csv, yaml, inspect, json
import math
import pandas as pd
import numpy as np
from scipy.io import mmread
from scipy.stats import chi2

from shapely.geometry import shape, mapping
from shapely.affinity import scale as shapely_scale


from cartloader.utils.minimake import minimake
from cartloader.utils.utils import cmd_separator, scheck_app, create_custom_logger, flexopen, unquote_str, smartsort, write_dict_to_file, load_file_to_dict, scheck_app
from cartloader.scripts.sge_convert import extract_unit2px_from_json
from cartloader.scripts.import_xenium_cell import process_cluster_csv, read_de_csv, write_de_tsv, write_cmap_tsv, tile_csv_into_pmtiles, make_factor_dict, write_umap_tsv, umap_tsv2pmtiles, umap_tsv2png, umap_tsv2indpng

def parse_arguments(_args):
    """
    Parse command-line arguments.
    """
    repo_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

    parser = argparse.ArgumentParser(
        prog=f"cartloader {inspect.getframeinfo(inspect.currentframe()).function}",
        description="Import square binned segmentation results from Visium HD Space Ranger outputs into PMTiles (optional catalog update)"
    )
    run_params = parser.add_argument_group("Run Options", "Execution controls for generating and running the Makefile")
    run_params.add_argument('--threads', type=int, default=4, help='Maximum number of threads per job (for tippecanoe) (default: 4)')
    run_params.add_argument('--log', action='store_true', default=False, help='Write logs to a file alongside outputs')
    run_params.add_argument('--log-suffix', type=str, default=".log", help='Log filename suffix; final path is <outprefix>_importxenium<suffix> (default: .log)')

    cmd_params = parser.add_argument_group("Commands", "Select actions to perform")
    cmd_params.add_argument('--update-catalog', action='store_true', default=False, help='(Optional) Update an existing catalog.yaml generated by run_cartload2')

    inout_params = parser.add_argument_group("Input/Output Parameters", 'Two input modes: 1) JSON — set --in-json with keys (CELL_FEATURE_MEX, CELL_GEOJSON, CLUSTER, DE); 2) Manual — set --in-dir and provide locations, see "Manual Input Parameters"')
    inout_params.add_argument('--in-json', type=str, help='Path to input JSON with paths for square bins, clusters, and differential expression.')
    inout_params.add_argument('--outprefix', type=str, required=True, help='Output prefix')
    inout_params.add_argument('--id', type=str, help='Identifier for the cell factor; if omitted, uses basename of --outprefix')
    inout_params.add_argument('--name', type=str, help='Display name for the cell factor; if omitted, uses basename of --outprefix')
    inout_params.add_argument('--bin-size', type=int, required=True, help='Binning size in um used in Space Ranger (8 or 16 is typical for Visium HD)')

    aux_inout_params = parser.add_argument_group("Manual Input Files Parameters", "Manually specify input directory (--in-dir) and input file locations under --in-dir")
    aux_inout_params.add_argument('--in-dir', type=str, help='Input directory containing the Space Ranger binned output files.')
    aux_inout_params.add_argument('--parquet', type=str, default="spatial/tissue_positions.parquet", help='Location of parquet files containing the location of bins')
    aux_inout_params.add_argument('--csv-clust', type=str, default="analysis/clustering/gene_expression_graphclust/clusters.csv", help='Location of CSV with cell cluster assignments under --in-dir (default: analysis/clustering/gene_expression_graphclust/clusters.csv)')
    aux_inout_params.add_argument('--csv-diffexp', type=str, default="analysis/diffexp/gene_expression_graphclust/differential_expression.csv", help='Location of CSV with differential expression results under --in-dir (default: analysis/diffexp/gene_expression_graphclust/differential_expression.csv)')
    # - scaling
    aux_inout_params.add_argument('--scale-json', type=str, default="spatial/scalefactors_json.json", help=f'Location of scale JSON under --in-dir. If set, defaults --units-per-um from microns_per_pixel in this JSON file (default: spatial/scalefactors_json.json")')
    aux_inout_params.add_argument('--units-per-um', type=float, default=1, help='Coordinate units per µm in inputs (default: 1).')
    # - UMAP
    aux_inout_params.add_argument('--csv-umap', type=str, default="analysis/pca/gene_expression_10_components/projection.csv", help='Location of CSV with UMAP results under --in-dir (default: analysis/pca/gene_expression_10_components/projection.csv')

    aux_colnames_params = parser.add_argument_group("Auxiliary Colname Parameters", "Override column names for input files")
    aux_colnames_params.add_argument('--pos-colname-barcode', type=str, default='barcode', help='Column name for bin barcode in --pos-parquet (default: barcode)')
    aux_colnames_params.add_argument('--pos-colname-x', type=str, default='pxl_col_in_fullres', help='Column name for X coordinates in --pos-parquet (default: pxl_col_in_fullres)')
    aux_colnames_params.add_argument('--pos-colname-y', type=str, default='pxl_row_in_fullres', help='Column name for Y coordinates in --pos-parquet (default: pxl_row_in_fullres)')
    aux_colnames_params.add_argument('--clust-colname-barcode', type=str, default='Barcode', help='Column name for bin barcode in --csv-clust (default: Barcode)')
    aux_colnames_params.add_argument('--clust-colname-cluster', type=str, default='Cluster', help='Column name for cluster label in --csv-clust (default: Cluster)')
    aux_colnames_params.add_argument('--umap-colname-barcode', type=str, default='Barcode', help='Column name for bin barcode in --csv-umap (default: Barcode)')
    aux_colnames_params.add_argument('--umap-colname-x', type=str, default='UMAP-1', help='Column name for UMAP X coordinate in --csv-umap (default: UMAP-1)')
    aux_colnames_params.add_argument('--umap-colname-y', type=str, default='UMAP-2', help='Column name for UMAP Y coordinate in --csv-umap (default: UMAP-2)')

    aux_conv_params = parser.add_argument_group("Auxiliary PMTiles Conversion Parameters")
    aux_conv_params.add_argument('--min-zoom', type=int, default=10, help='Minimum zoom level (default: 10)')
    aux_conv_params.add_argument('--max-zoom', type=int, default=18, help='Maximum zoom level (default: 18)')
    aux_conv_params.add_argument('--umap-min-zoom', type=int, default=0, help='Minimum zoom level for UMAP (default: 0)')
    aux_conv_params.add_argument('--umap-max-zoom', type=int, default=18, help='Maximum zoom level for UMAP (default: 18)')
    aux_conv_params.add_argument('--max-tile-bytes', type=int, default=5000000, help='Maximum bytes for each tile in PMTiles (default: 5000000)')
    aux_conv_params.add_argument('--max-feature-counts', type=int, default=500000, help='Max feature limits per tile in PMTiles (default: 500000)')
    aux_conv_params.add_argument('--preserve-point-density-thres', type=int, default=1024, help='Threshold for preserving point density in PMTiles (default: 1024)')

    aux_params = parser.add_argument_group("Auxiliary Parameters", "Advanced settings; defaults work for most cases")    
    aux_params.add_argument('--use-parquet-tools', action='store_true', help='Use parquet-tools instead of polars/pigz for parquet to csv conversion (default: False). parquet-tools may be slower for large files.')
    aux_params.add_argument('--tsv-cmap', type=str, default=f"{repo_dir}/assets/fixed_color_map_60.tsv", help=f'Location of TSV with color mappings for clusters (default: {repo_dir}/assets/fixed_color_map_60.tsv)')
    aux_params.add_argument('--de-max-pval', type=float, default=0.01, help='Maximum p-value for differential expression (default: 0.01)')
    aux_params.add_argument('--de-min-fc', type=float, default=1.2, help='Minimum fold change for differential expression (default: 1.2)')
    aux_params.add_argument('--catalog-yaml', type=str, help='Path to catalog.yaml to update (used with --update-catalog; default: <in-dir>/catalog.yaml)')
    aux_params.add_argument('--keep-intermediate-files', action='store_true', default=False, help='Keep intermediate output files')
    aux_params.add_argument('--tmp-dir', type=str, help='Temporary directory for intermediate files (default: out-dir/tmp or /tmp if specified)')

    env_params = parser.add_argument_group("Env Parameters", "Tool paths (override defaults if needed)")
    env_params.add_argument('--tippecanoe', type=str, default=f"{repo_dir}/submodules/tippecanoe/tippecanoe", help='Path to tippecanoe binary (default: <cartloader_dir>/submodules/tippecanoe/tippecanoe)')
    env_params.add_argument('--parquet-tools', type=str, default="parquet-tools", help='Path to parquet-tools binary. Required if a Parquet file is provided. (default: parquet-tools)')
    env_params.add_argument('--gzip', type=str, default="gzip", help='Path to gzip binary (default: gzip)')
    env_params.add_argument('--pigz', type=str, default="pigz", help='Path to pigz binary (default: pigz)')
    env_params.add_argument('--pigz-threads', type=int, default=4, help='Number of threads for pigz (default: 4)')
    env_params.add_argument('--R', type=str, default="R", help='Path to R binary (default: R).')

    if len(_args) == 0:
        parser.print_help()
        sys.exit(1)

    args = parser.parse_args(_args)

    # Sanity check: JSON mode and manual mode are mutually exclusive
    if args.in_json and args.in_dir:
        parser.error("Cannot enable both JSON mode (--in-json) and manual input mode (--in-dir and/or --parquet/--csv-clust/--csv-diffexp). Choose one.")

    return args

def import_visiumhd_square(_args):
    """
    Import square binned output results from Space Ranger output
    """

    # parse argument
    args=parse_arguments(_args)

    logger = create_custom_logger(__name__, args.outprefix + "_import_visium_square" + args.log_suffix if args.log else None)
    logger.info("Analysis Started")

    assert args.bin_size > 0, "--bin-size must be a positive integer"

    # create output directory if needed
    out_dir = os.path.dirname(args.outprefix)
    out_base = os.path.basename(args.outprefix)
    if not os.path.exists(out_dir) and out_dir != "":
        os.makedirs(out_dir, exist_ok=True)

    if args.tmp_dir is None:
        args.tmp_dir = os.path.join(out_dir, "tmp")
        if not os.path.exists(args.tmp_dir):
            os.makedirs(args.tmp_dir, exist_ok=True)

    # read in_json if provided 
    label = f"GRID_{args.bin_size}um"

    scale_json = None
    temp_fs = []

    if args.in_json is not None:
        assert os.path.exists(args.in_json), f"File not found: {args.in_json} (--in-json)"
        raw_data = load_file_to_dict(args.in_json)
        scale_json = raw_data.get(label, {}).get("SCALE", None)
        bin_data = raw_data.get(label, {})
    else:
        if args.in_dir is None:
            raise ValueError("--in-dir is required when --in-json is not provided")
        
        scale_json =  os.path.join(args.in_dir, args.scale_json) if args.scale_json else None
        
        bin_data = {
            "POSITION": os.path.join(args.in_dir, args.parquet),
            "CLUSTER": os.path.join(args.in_dir, args.csv_clust),
            "DE": os.path.join(args.in_dir, args.csv_diffexp),
            "UMAP_PROJ": f"{args.in_dir}/{args.csv_umap}"
        }

    if scale_json is not None:
        assert os.path.exists(scale_json), f"File not found: {scale_json} (--scale-json)"
        args.units_per_um = extract_unit2px_from_json(scale_json)
        logger.info(f"Setting --units-per-um = {args.units_per_um} from {scale_json}")
    else:
        logger.warning(f"No scale JSON provided; assuming --units-per-um = {args.units_per_um}")

    ## convert parquet files to csv
    parquet_path = bin_data["POSITION"]
    parquet_csv_path = args.outprefix + ".parquet.csv.gz"
    if args.use_parquet_tools:
        par2csv_cmd = f"{args.parquet_tools} csv {parquet_path} |  {args.gzip} -c > {parquet_csv_path}"
    else:
        par2csv_cmd = f"cartloader parquet_to_csv_rapid --in-parquet {parquet_path} --out-csv-gz {parquet_csv_path} --pigz {args.pigz} --threads {args.pigz_threads}"
    temp_fs.append(parquet_csv_path)

    logger.info(f"  * Converting {parquet_path} from parquet to CSV: {par2csv_cmd}")
    result = subprocess.run(par2csv_cmd, shell=True, capture_output=True)
    if result.returncode != 0:
        logger.error(f"Command {par2csv_cmd}\nfailed with error: {result.stderr.decode()}")
        sys.exit(1)

    ## Import cluster/DE
    clust_in = bin_data.get("CLUSTER", None)
    assert clust_in is not None, ('Path not provided: "CLUSTER" in --in-json' if args.in_json is not None else 'Path not provided: --csv-clust')
    assert os.path.exists(clust_in), (f'File not found: {clust_in} ("CLUSTER" in --in-json)' if args.in_json is not None else f'File not found: {clust_in} (--csv-clust)')

    logger.info(f"Loading cell cluster data from {clust_in}")
    sorted_clusters, cluster2idx, bcd2clusteridx = process_cluster_csv(
        clust_in,
        barcode_col=args.clust_colname_barcode,
        cluster_col=args.clust_colname_cluster
    )
    logger.info(f"  * Loaded {len(bcd2clusteridx)} barcodes with {len(sorted_clusters)} clusters (min: {sorted_clusters[0]}, max: {sorted_clusters[-1]})")
        
    ## read/write DE results
    de_in = bin_data.get("DE", None)
    assert de_in is not None, ('Path not provided: "DE" in --in-json' if args.in_json is not None else 'Path not provided: --csv-diffexp')
    assert os.path.exists(de_in), (f'File not found: {de_in} ("DE" in --in-json)' if args.in_json is not None else f'File not found: {de_in} (--csv-diffexp)')        

    logger.info(f"  * Reading DE results from {de_in}")
    clust2genes=read_de_csv(de_in, cluster2idx, args.de_min_fc, args.de_max_pval)

    ## write DE results
    de_out=f"{args.outprefix}-bulk-de.tsv"
    logger.info(f"  * Writing DE results for {len(clust2genes)} clusters) to {de_out}")
    write_de_tsv(clust2genes, de_out, sorted_clusters)

    ## write the color map
    assert os.path.exists(args.tsv_cmap), f"File not found: {args.tsv_cmap} (--tsv-cmap)"        

    cmap_out=f"{args.outprefix}-rgb.tsv"
    logger.info(f"  * Writing color map from {args.tsv_cmap} to {cmap_out}")
    write_cmap_tsv(cmap_out, args.tsv_cmap, sorted_clusters)

    # Process segmented bins
    geojson_out=f"{args.outprefix}.geojson"
    with flexopen(parquet_csv_path, 'rt') as rf, flexopen(geojson_out, 'wt') as wf:
        hdrs = rf.readline().rstrip().split(',')
        col2idx = {c: i for i, c in enumerate(hdrs)}
        try:
            ibcd = col2idx[args.pos_colname_barcode]
            ix = col2idx[args.pos_colname_x]
            iy = col2idx[args.pos_colname_y]
        except KeyError as e:
            raise ValueError(f"Required columns not found in {parquet_path}: {e}")
        logger.info(f"  * Reading binned data from {parquet_path} and writing to {geojson_out}")
        for line in rf:
            toks = line.rstrip().split(',')
            if len(toks) < len(hdrs):
                logger.warning(f"Skipping line with unexpected number of columns (expected {len(hdrs)}, got {len(toks)}): {line.strip()}")
                continue
            bcd = toks[ibcd]
            x = float(toks[ix])/args.units_per_um
            y = float(toks[iy])/args.units_per_um
            clusteridx = bcd2clusteridx.get(bcd, "NA")
            vertices = []
            half_bin = args.bin_size / 2.0
            vertices.append(f"[{(x - half_bin):.3f},{(y - half_bin):.3f}]")
            vertices.append(f"[{(x - half_bin):.3f},{(y + half_bin):.3f}]")
            vertices.append(f"[{(x + half_bin):.3f},{(y + half_bin):.3f}]")
            vertices.append(f"[{(x + half_bin):.3f},{(y - half_bin):.3f}]")
            wf.write(f'{{"type": "Feature", "geometry": {{"type": "Polygon", "coordinates": [[{",".join(vertices)}]]}}, "properties": {{"barcode": "{bcd}", "topK": "{clusteridx}"}}}}\n')
        logger.info(f"  * Finished writing {geojson_out}")

        bins_pmtiles = f"{args.outprefix}.pmtiles"
        logger.info(f"  * Generating PMTiles from cell geometry data into cell pmtiles: {bins_pmtiles}")
        tile_csv_into_pmtiles(geojson_out, bins_pmtiles, args, logger, no_dup=True)
        temp_fs.append(geojson_out)

    # UMAP
    scheck_app(args.R)
    umap_in = bin_data.get("UMAP_PROJ", None)
    umap_tsv_out = f"{args.outprefix}-umap.tsv.gz"
    umap_pmtiles = f"{args.outprefix}-umap.pmtiles"

    assert umap_in is not None, (f'Path not provided: "UMAP_PROJ" in field "{label}" --in-json' if args.in_json is not None else 'Path not provided: --csv-umap')
    assert os.path.exists(umap_in), (f'File not found: {umap_in} ("UMAP_PROJ" in field "{label}" --in-json)' if args.in_json is not None else f'File not found: {umap_in} (--csv-umap)')

    logger.info(f"Processing UMAP projection from {umap_in}")
    write_umap_tsv(umap_in, umap_tsv_out, bcd2clusteridx, args)

    logger.info(f"  * Generated PMTiles for UMAP projection:{umap_pmtiles}")
    umap_tsv2pmtiles(umap_tsv_out, umap_pmtiles, args)

    logger.info(f"  * UMAP Visualization for all factors...")
    umap_tsv2png(umap_tsv_out, args.outprefix, cmap_out, title = f"Square Bin {args.bin_size}um")

    logger.info(f"  * UMAP Visualization (plot for individual factors; colorized by cluster)...")
    umap_tsv2indpng(umap_tsv_out, args.outprefix, cmap_out, mode="binary", title = f"Square Bin {args.bin_size}um")

    # JSON/YAML (always summary)
    factor_id = out_base if args.id is None else args.id
    factor_name = out_base if args.name is None else args.name
    pmtiles_keys=[]
    if os.path.exists(f"{args.outprefix}.pmtiles"):
        pmtiles_keys.append(f"sq{args.bin_size:03d}")
    
    if os.path.exists(f"{args.outprefix}-umap.pmtiles") and os.path.exists(f"{args.outprefix}-umap.tsv.gz") and os.path.exists(f"{args.outprefix}.umap.png") and os.path.exists(f"{args.outprefix}.umap.single.binary.png"):
        umap_src = True
    else:
        umap_src = False

    print(pmtiles_keys)
    out_assets_f=f"{args.outprefix}_assets.json"
    logger.info(f"Summarizing assets information into {out_assets_f}")
    new_factor = make_factor_dict(factor_id, factor_name, args.outprefix, factor_type="square", pmtiles_keys=pmtiles_keys, umap_src=umap_src)
    write_dict_to_file(new_factor, out_assets_f, check_equal=True)

    if args.update_catalog:
        ## read the input catalog.yaml file
        if args.catalog_yaml is None:
            args.catalog_yaml = os.path.join(args.in_dir, "catalog.yaml")
        
        logger.info(f"Updating catalog YAML file: {args.catalog_yaml}")

        ## load the YAML file
        with open(args.catalog_yaml, 'r') as f:
            catalog = yaml.load(f, Loader=yaml.FullLoader)  # Preserves order

        ## add files to the catalog
        new_factor = make_factor_dict(factor_id, factor_name, out_base, factor_type="square", pmtiles_keys=pmtiles_keys, umap_src=umap_src)
        if "factors" not in catalog["assets"]:
            raise ValueError("No factors found in the catalog.yaml file. Check if the file is correct.")
        catalog["assets"]["factors"].append(new_factor)

        out_yaml = f"{args.outprefix}-catalog.yaml"
        with open(out_yaml, 'w') as f:
            yaml.dump(catalog, f, Dumper=yaml.SafeDumper, default_flow_style=False, sort_keys=False)
        logger.info(f"Successfully wrote the catalog.yaml file: {out_yaml}")

    ## clean the temp files
    if not args.keep_intermediate_files:
        logger.info(f"Cleaning intermediate files")
        if len(temp_fs) >0:
            for temp_f in temp_fs:
                if os.path.exists(temp_f):
                    os.remove(temp_f)

    logger.info("Analysis Finished")

if __name__ == "__main__":
    # Get the base file name without extension
    script_name = os.path.splitext(os.path.basename(__file__))[0]

    # Dynamically get the function based on the script name
    func = getattr(sys.modules[__name__], script_name)

    # Call the function with command line arguments
    func(sys.argv[1:])
