import sys, os, gzip, argparse, logging, warnings, shutil, subprocess, ast, csv, yaml, inspect, json
import pandas as pd
import numpy as np
from scipy.io import mmread
from scipy.stats import chi2

from shapely.geometry import shape, mapping

from cartloader.utils.minimake import minimake
from cartloader.utils.utils import cmd_separator, scheck_app, create_custom_logger, flexopen, unquote_str, smartsort, write_dict_to_file, load_file_to_dict
from cartloader.scripts.import_xenium_cell import process_cluster_csv, read_de_csv, write_de_tsv, write_cmap_tsv, tile_csv_into_pmtiles, make_factor_dict


def process_cell_geojson_w_mtx(cells_geojson, cell_ftr_mex, cells_out, bcd2clusteridx):
    # Load barcodes
    bcd_path = os.path.join(cell_ftr_mex, "barcodes.tsv.gz")
    assert os.path.exists(bcd_path), f"Missing barcode file in CELL_FEATURE_MEX: {bcd_path}"
    df_bcd_idx = pd.read_csv(bcd_path, header=None, names=["cell_id"])
    df_bcd_idx["barcode_index"] = df_bcd_idx.index + 1  # 1-based indexing (MatrixMarket style)

    # Map barcode to cluster idx
    df_bcd_idx["clusteridx"] = df_bcd_idx["cell_id"].map(bcd2clusteridx)

    # Load mtx
    mtx_path = os.path.join(cell_ftr_mex, "matrix.mtx.gz")
    assert os.path.exists(mtx_path), f"Missing matrix expression file in CELL_FEATURE_MEX: {mtx_path}"
    with gzip.open(mtx_path, "rt") as f:
        matrix = mmread(f).tocsr()

    # Compute UMI counts per barcode (i.e., column-wise sum)
    barcode_sums = matrix.sum(axis=0).A1
    df_bcd_ct = pd.DataFrame({
        "barcode_index": range(1, len(barcode_sums) + 1),
        "count": barcode_sums
    })

    # Load convert cells_geojson into csv
    with open(cells_geojson, "r") as f:
        cell_data = json.load(f)
    df_bcd_xy = pd.DataFrame(
        {
            "cell_id": f"cellid_{f['properties']['cell_id']:09d}-1",
            "lon": shape(f["geometry"]).centroid.x,
            "lat": shape(f["geometry"]).centroid.y,
        }
        for f in cell_data["features"]
    )

    # Merge and select final columns
    df_bcd = (
        df_bcd_ct
        .merge(df_bcd_idx, on="barcode_index", how="left")
        .merge(df_bcd_xy, on="cell_id", how="left")
        [["lon", "lat", "cell_id", "count", "clusteridx"]]
        .rename(columns={"clusteridx": "topK"})
    )
    df_bcd["topK"] = df_bcd["clusteridx"].apply(lambda x: str(x) if pd.notna(x) else "NA")
    df_bcd.to_csv(cells_out)

def process_boundaries_geojson(input_geojson, output_geojson, bcd2clusteridx):
    assert os.path.exists(input_geojson), f"The input GEOJSON file doesn't exist: {input_geojson}"
    with open(input_geojson) as f:
        geojson = json.load(f)

    for feature in geojson["features"]:
        raw_id = feature["properties"]["cell_id"]
        formatted_id = f"cellid_{raw_id:09d}-1"
        clusteridx = bcd2clusteridx.get(formatted_id, "NA")
        feature["properties"] = {
            "cell_id": formatted_id,
            "topK": str(clusteridx)
        }

    with open(output_geojson, "w") as f:
        json.dump({"type": "FeatureCollection", "features": geojson["features"]}, f, indent=2)

def parse_arguments(_args):
    """
    Parse command-line arguments.
    """
    repo_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))

    parser = argparse.ArgumentParser(prog=f"cartloader {inspect.getframeinfo(inspect.currentframe()).function}", description="Import cell segmentation results from Xenium Ranger output")

    cmd_params = parser.add_argument_group("Commands", "Commands to run together")
    cmd_params.add_argument('--all', action='store_true', default=False, help='Run all commands (cells, boundaries, summary)')
    cmd_params.add_argument('--cells', action='store_true', default=False, help='Add segmented cells to PMTiles output as factors')
    cmd_params.add_argument('--boundaries', action='store_true', default=False, help='Add segmented cell bounaries to PMTiles output')
    cmd_params.add_argument('--summary', action='store_true', default=False, help='Generate a JSON file summarizing parameters and output paths.')
    cmd_params.add_argument('--update-catalog', action='store_true', default=False, help='Update an existing catalog YAML file, which is generated by run_cartload2* (NOT included in --all)')

    inout_params = parser.add_argument_group("Input/Output Parameters", """
                                             Input/output directory/files. There are two ways to specify the input files. 
                                             One is to use --in-json to provide the paths. 
                                             The other is to use --indir with auxiliary input parameters (--csv-*) to specify input paths """)
    inout_params.add_argument('--in-json', type=str, help="Provide a json file specified the path for the cells, boundaries, cluster, diffexp")
    inout_params.add_argument('--indir', type=str, help='Input directory containing the Xenium Ranger output files. If --in-json is specified, skip this argument')
    inout_params.add_argument('--outprefix', type=str, help='Prefix of output files')
    inout_params.add_argument('--id', type=str, help='Identifier of the factor')
    inout_params.add_argument('--name', type=str, help='Name of the factor')

    conv_params = parser.add_argument_group("Parameters for pmtiles conversion")
    conv_params.add_argument('--min-zoom', type=int, default=10, help='Minimum zoom level (default: 10)')
    conv_params.add_argument('--max-zoom', type=int, default=18, help='Maximum zoom level (default: 18)')
    conv_params.add_argument('--max-tile-bytes', type=int, default=5000000, help='Maximum bytes for each tile in PMTiles (default: 5000000)')
    conv_params.add_argument('--max-feature-counts', type=int, default=500000, help='Max feature limits per tile in PMTiles (default: 500000)')
    conv_params.add_argument('--preserve-point-density-thres', type=int, default=1024, help='Threshold for preserving point density in PMTiles (default: 1024)')

    run_params = parser.add_argument_group("Run Options", "Run options for GNU Make")
    run_params.add_argument('--threads', type=int, default=4, help='Maximum number of threads per job (for tippecanoe)')
    run_params.add_argument('--log', action='store_true', default=False, help='Write log to file')
    run_params.add_argument('--log-suffix', type=str, default=".log", help='The suffix for the log file (appended to the output directory). Default: .log')

    aux_params = parser.add_argument_group("Auxiliary Parameters", "Auxiliary parameters (using default is recommended)")
    aux_params.add_argument('--catalog-yaml', type=str, help='Path to YAML file to update when --update-catalog is enabled.') #If the file is outside the output directory, output files will be copied to its directory.')
    aux_params.add_argument('--col-rename', type=str, nargs='+', help='Columns to rename in the output file. Format: old_name1:new_name1 old_name2:new_name2 ...')
    aux_params.add_argument('--geojson-cells', type=str, default="segmented_outputs/cell_segmentations.geojson", help='Location of the GEOJSON file containing cell locations in the --indir (default: segmented_outputs/cell_segmentations.geojson)')
    aux_params.add_argument('--mtx-cell-ftr', type=str, default="segmented_outputs/filtered_feature_cell_matrix", help='Location of the cell feature matrix in MTX format (default: segmented_outputs/filtered_feature_cell_matrix)')
    aux_params.add_argument('--csv-clust', type=str, default="analysis/clustering/gene_expression_graphclust/clusters.csv", help='Location of the CSV file containing cell cluster assignments in the --indir (default: analysis/clustering/gene_expression_graphclust/clusters.csv)')
    aux_params.add_argument('--csv-diffexp', type=str, default="analysis/diffexp/gene_expression_graphclust/differential_expression.csv", help='Location of the CSV file with differential expression results in the --indir (default: analysis/diffexp/gene_expression_graphclust/differential_expression.csv)')
    aux_params.add_argument('--tsv-cmap', type=str, default=f"{repo_dir}/assets/fixed_color_map_60.tsv", help=f'Location of the TSV file with color mappings for clusters in the --indir (default: {repo_dir}/assets/fixed_color_map_60.tsv)')
    aux_params.add_argument('--de-max-pval', type=float, default=0.01, help='Maximum p-value threshold for differential expression (default: 0.01)')
    aux_params.add_argument('--de-min-fc', type=float, default=1.2, help='Minimum fold change threshold for differential expression (default: 1.2)')
    #aux_params.add_argument('--keep-intermediate-files', action='store_true', default=False, help='Keep intermediate output files (default: False)')
    aux_params.add_argument('--tmp-dir', type=str, help='Temporary directory for intermediate files (default: out-dir/tmp or /tmp if specified)')

    env_params = parser.add_argument_group("ENV Parameters", "Environment parameters for the tools")
    env_params.add_argument('--tippecanoe', type=str, default=f"{repo_dir}/submodules/tippecanoe/tippecanoe", help='Path to tippecanoe binary (default: <cartloader_dir>/submodules/tippecanoe/tippecanoe)')
    env_params.add_argument('--parquet-tools', type=str, default="parquet-tools", help='Path to parquet-tools binary. Required if a parquet file is provided to --csv-cells (defaults: parquet-tools)')

    if len(_args) == 0:
        parser.print_help()
        sys.exit(1)

    return parser.parse_args(_args)

def import_visiumhd_cell(_args):
    """
    Import cell segmentation results from Xenium Ranger output
    """

    # parse argument
    args=parse_arguments(_args)

    logger = create_custom_logger(__name__, args.outprefix + "_importxenium" + args.log_suffix if args.log else None)
    logger.info("Analysis Started")

    if args.all:
        args.cells = True
        args.boundaries = True
        args.summary = True
        # args.update_catalog = True
    
    if not args.cells and not args.boundaries and not args.summary and not args.update_catalog:
        raise ValueError("At least one action should be activated.")

    # create output directory if needed
    out_dir = os.path.dirname(args.outprefix)
    out_base = os.path.basename(args.outprefix)
    if not os.path.exists(out_dir) and out_dir != "":
        os.makedirs(out_dir, exist_ok=True)

    if args.tmp_dir is None:
        args.tmp_dir = os.path.join(out_dir, "tmp")
        if not os.path.exists(args.tmp_dir):
            os.makedirs(args.tmp_dir, exist_ok=True)

    #temp_fs=[]

    # read in_json if provided 
    if args.in_json is not None:
        assert os.path.exists(args.in_json), f"The input json file doesn't exist: {args.in_json}"
        cellbounds=load_file_to_dict(args.in_json)
    else:
        cellbounds={
            "CELL_GEOJSON": f"{args.indir}/{args.geojson_cells}",
            "CELL_FEATURE_MEX":  f"{args.indir}/{args.mtx_cell_ftr}",
            "BOUNDARY": f"{args.indir}/{args.csv_boundaries}",
            "CLUSTER": f"{args.indir}/{args.csv_clust}",
            "DE": f"{args.indir}/{args.csv_diffexp}",
        }

    # Cluster/DE
    if args.cells or args.boundaries:
        clust_in=cellbounds.get("CLUSTER", None)
        assert os.path.exists(clust_in), f"The input cluster file doesn't exist {clust_in}"

        logger.info(f"Loading cell cluster data from {clust_in}")
        sorted_clusters, cluster2idx, bcd2cluster, bcd2clusteridx=process_cluster_csv(clust_in)
        logger.info(f"  * Loaded {len(bcd2cluster)} cells\n")
        
        ## read/write DE results
        de_in=cellbounds.get("DE", None)
        assert os.path.exists(de_in), f"The input differentially expressed profile file doesn't exist {de_in}"        

        logger.info(f"  * Reading DE results from {de_in}")
        clust2genes=read_de_csv(de_in, cluster2idx, args.de_min_fc, args.de_max_pval)

        ## write DE results
        de_out=f"{args.outprefix}-cells-bulk-de.tsv"
        logger.info(f"  * Writing DE results for {len(clust2genes)} clusters) to {de_out}")
        write_de_tsv(clust2genes, de_out, sorted_clusters)

        ## write the color map
        cmap_out=f"{args.outprefix}-rgb.tsv"
        assert os.path.exists(args.tsv_cmap), f"The input color map doesn't exist {args.tsv_cmap}"        

        logger.info(f"  * Writing color map from {args.tsv_cmap} to {cmap_out}")
        write_cmap_tsv(cmap_out, args.tsv_cmap, sorted_clusters)

    # Process segmented calls 
    if args.cells:
        cells_in=cellbounds.get("CELL_GEOJSON", None)
        assert os.path.exists(cells_in), f"The input cells GEOJSON file doesn't exist: {cells_in}"

        cell_ftr_mex = cellbounds.get("CELL_FEATURE_MEX", None)        
        assert cell_ftr_mex and os.path.exists(cell_ftr_mex), "Cell GEOJSON file must be used along with CELL_FEATURE_MEX"
        
        logger.info(f"Processing cell information from {cells_in}; {cell_ftr_mex}")

        # convert cell geojson into csv format with cell_id, x, y, count
        cells_out=f"{args.outprefix}-cells.csv"
        logger.info(f"  * Reading cell data from {cells_in}; {cell_ftr_mex} and extracting geometry to {cells_out}")
        process_cell_geojson_w_mtx(cells_in, cell_ftr_mex, cells_out, bcd2clusteridx)

        cells_pmtiles = f"{args.outprefix}-cells.pmtiles"
        logger.info(f"  * Generating PMTiles from cell geometry data into cell pmtiles: {cells_pmtiles}")
        tile_csv_into_pmtiles(cells_out, cells_pmtiles, args, logger, no_dup=True)
        #temp_fs.append(cells_out)

    # Process cell boundaries
    if args.boundaries:
        bound_in=cellbounds.get("BOUNDARY", None)
        assert os.path.exists(bound_in), f"The input cell boundary files doesn't exist {bound_in}"

        logger.info(f"Processing cell boundary information from {bound_in}")
        ## read the cell CSV files
        bound_out=f"{args.outprefix}-boundaries.geojson"
        logger.info(f"  * Reading cell boundary data from {bound_in} and extracting geometry to {bound_out}")
        process_boundaries_geojson(bound_in, bound_out, bcd2clusteridx)

        ## Run the tippecanoe command
        bound_pmtiles = f"{args.outprefix}-boundaries.pmtiles"
        logger.info(f"  * Generating PMTiles from boundary geometry data into boundary pmtiles: {bound_pmtiles}")
        tile_csv_into_pmtiles(bound_out, bound_pmtiles, args, logger, no_dup=False)
        #temp_fs.append(bound_out)
    
    # JSON/YAML
    if args.summary or args.update_catalog:
        ## add the new factor to the catalog
        factor_id = out_base if args.id is None else args.id
        factor_name = out_base if args.name is None else args.name
        pmtiles_keys=[]
        if os.path.exists(f"{args.outprefix}-cells.pmtiles"):
            pmtiles_keys.append("cells")
        if os.path.exists(f"{args.outprefix}-boundaries.pmtiles"):
            pmtiles_keys.append("boundaries")

    if args.summary:
        out_assets_f=f"{args.outprefix}_assets.json"
        logger.info(f"Summarizing assets information into {out_assets_f}")
        new_factor = make_factor_dict(factor_id, factor_name, args.outprefix, pmtiles_keys)
        write_dict_to_file(new_factor, out_assets_f, check_equal=True)

    if args.update_catalog:
        ## read the input catalog.yaml file
        if args.catalog_yaml is None:
            args.catalog_yaml = os.path.join(args.indir, "catalog.yaml")
        
        logger.info(f"Updating catalog YAML file:{args.catalog_yaml}")

        ## load the YAML file
        with open(args.catalog_yaml, 'r') as f:
            catalog = yaml.load(f, Loader=yaml.FullLoader)  # Preserves order

        ## add files to the catalog
        new_factor = make_factor_dict(factor_id, factor_name, out_base, pmtiles_keys)
        if "factors" not in catalog["assets"]:
            raise ValueError("No factors found in the catalog.yaml file. Check if the file is correct.")
        catalog["assets"]["factors"].append(new_factor)

        ## write the updated catalog.yaml file
        ##TO-DO: 
        # - catalog_yaml should be within the same dir with the output assets
        # - could update catalog directly without creating a new catalog file

        out_yaml = f"{args.outprefix}-catalog.yaml"
        with open(out_yaml, 'w') as f:
            yaml.dump(catalog, f, Dumper=yaml.SafeDumper, default_flow_style=False, sort_keys=False)
        logger.info(f"Successfully wrote the catalog.yaml file: {out_yaml}")

    ## clean the temp files
    # if not args.keep_intermediate_files:
    #     logger.info(f"Cleaning intermediate files")
    #     if len(temp_fs) >0:
    #         for temp_f in temp_fs:
    #             if os.path.exists(temp_f):
    #                 os.remove(temp_f)


    logger.info("Analysis Finished")

if __name__ == "__main__":
    # Get the base file name without extension
    script_name = os.path.splitext(os.path.basename(__file__))[0]

    # Dynamically get the function based on the script name
    func = getattr(sys.modules[__name__], script_name)

    # Call the function with command line arguments
    func(sys.argv[1:])
