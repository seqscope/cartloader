import sys, os, gzip, argparse, logging, warnings, shutil, subprocess, ast, csv, yaml, inspect, json
import pandas as pd
import numpy as np
from scipy.io import mmread
from scipy.stats import chi2

from shapely.geometry import shape, mapping

from cartloader.utils.minimake import minimake
from cartloader.utils.utils import cmd_separator, scheck_app, create_custom_logger, flexopen, unquote_str, smartsort, write_dict_to_file, load_file_to_dict
from cartloader.scripts.import_xenium_cell import process_cluster_csv, read_de_csv, write_de_tsv, write_cmap_tsv, tile_csv_into_pmtiles, make_factor_dict


def process_cell_geojson_w_mtx(cells_geojson, cell_ftr_mex, cells_out, bcd2clusteridx):
    # Load barcodes
    bcd_path = os.path.join(cell_ftr_mex, "barcodes.tsv.gz")
    df_bcd_idx = pd.read_csv(bcd_path, header=None, names=["cell_id"])
    df_bcd_idx["barcode_index"] = df_bcd_idx.index + 1  # 1-based indexing (MatrixMarket style)

    # Map barcode to cluster idx
    df_bcd_idx["clusteridx"] = df_bcd_idx["cell_id"].map(bcd2clusteridx)

    # Load mtx
    mtx_path = os.path.join(cell_ftr_mex, "matrix.mtx.gz")
    with gzip.open(mtx_path, "rt") as f:
        matrix = mmread(f).tocsr()

    # Compute UMI counts per barcode (i.e., column-wise sum)
    barcode_sums = matrix.sum(axis=0).A1
    df_bcd_ct = pd.DataFrame({
        "barcode_index": range(1, len(barcode_sums) + 1),
        "count": barcode_sums
    })

    # Load convert cells_geojson into csv
    with open(cells_geojson, "r") as f:
        cell_data = json.load(f)
    df_bcd_xy = pd.DataFrame(
        {
            "cell_id": f"cellid_{f['properties']['cell_id']:09d}-1",
            "lon": shape(f["geometry"]).centroid.x,
            "lat": shape(f["geometry"]).centroid.y,
        }
        for f in cell_data["features"]
    )

    # Merge and select final columns
    df_bcd = (
        df_bcd_ct
        .merge(df_bcd_idx, on="barcode_index", how="left")
        .merge(df_bcd_xy, on="cell_id", how="left")
        [["lon", "lat", "cell_id", "count", "clusteridx"]]
        .rename(columns={"clusteridx": "topK"})
    )
    df_bcd["topK"] = df_bcd["topK"].apply(lambda x: str(x) if pd.notna(x) else "NA")
    df_bcd.to_csv(cells_out)

def process_boundaries_geojson(input_geojson, output_geojson, bcd2clusteridx):
    with open(input_geojson) as f:
        geojson = json.load(f)

    for feature in geojson["features"]:
        raw_id = feature["properties"]["cell_id"]
        formatted_id = f"cellid_{raw_id:09d}-1"
        clusteridx = bcd2clusteridx.get(formatted_id, "NA")
        feature["properties"] = {
            "cell_id": formatted_id,
            "topK": str(clusteridx)
        }

    with open(output_geojson, "w") as f:
        json.dump({"type": "FeatureCollection", "features": geojson["features"]}, f, indent=2)

def parse_arguments(_args):
    """
    Parse command-line arguments.
    """
    repo_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

    parser = argparse.ArgumentParser(
        prog=f"cartloader {inspect.getframeinfo(inspect.currentframe()).function}",
        description="Import cell segmentation results from Visium HD Space Ranger outputs and package cells/boundaries into PMTiles (optional catalog update)"
    )
    run_params = parser.add_argument_group("Run Options", "Execution controls for generating and running the Makefile")
    run_params.add_argument('--threads', type=int, default=4, help='Maximum number of threads per job (for tippecanoe) (default: 4)')
    run_params.add_argument('--log', action='store_true', default=False, help='Write logs to a file alongside outputs')
    run_params.add_argument('--log-suffix', type=str, default=".log", help='Log filename suffix; final path is <outprefix>_importxenium<suffix> (default: .log)')

    cmd_params = parser.add_argument_group("Commands", "Select one or more actions to perform")
    cmd_params.add_argument('--all', action='store_true', default=False, help='Enable all actions: --cells, --boundaries, and --summary')
    cmd_params.add_argument('--cells', action='store_true', default=False, help='Import segmented cells and generate PMTiles')
    cmd_params.add_argument('--boundaries', action='store_true', default=False, help='Import segmented cell boundaries and generate GeoJSON')
    cmd_params.add_argument('--summary', action='store_true', default=False, help='Write a JSON summary of parameters and output paths')
    cmd_params.add_argument('--update-catalog', action='store_true', default=False, help='Update an existing catalog.yaml generated by run_cartload2 (not included in --all)')

    inout_params = parser.add_argument_group("Input/Output Parameters", 'Two input modes: 1) JSON — set --in-json with keys (CELL_FEATURE_MEX, CELL_GEOJSON, CLUSTER, DE); 2) Manual — set --indir and provide locations, see "Manual Input Parameters"')
    inout_params.add_argument('--in-json', type=str, help='Path to input JSON with paths for cells, boundaries, clusters, and differential expression.')
    inout_params.add_argument('--outprefix', type=str, required=True, help='Output prefix')
    inout_params.add_argument('--id', type=str, help='Identifier for the cell factor; if omitted, uses basename of --outprefix')
    inout_params.add_argument('--name', type=str, help='Display name for the cell factor; if omitted, uses basename of --outprefix')

    aux_inout_params = parser.add_argument_group("Manual Input Files Parameters", "Manually specify input directory (--indir) and input file locations under --indir")
    aux_inout_params.add_argument('--indir', type=str, help='Input directory containing the Space Ranger output files.')
    aux_inout_params.add_argument('--geojson-cells', type=str, default="segmented_outputs/cell_segmentations.geojson", help='Location of GEOJSON with cell locations under --indir (default: segmented_outputs/cell_segmentations.geojson)')
    aux_inout_params.add_argument('--mtx-cell', type=str, default="segmented_outputs/filtered_feature_cell_matrix", help='Directory location of the cell feature MatrixMarket files under --indir (default: segmented_outputs/filtered_feature_cell_matrix)')
    aux_inout_params.add_argument('--csv-clust', type=str, default="analysis/clustering/gene_expression_graphclust/clusters.csv", help='Location of CSV with cell cluster assignments under --indir (default: analysis/clustering/gene_expression_graphclust/clusters.csv)')
    aux_inout_params.add_argument('--csv-diffexp', type=str, default="analysis/diffexp/gene_expression_graphclust/differential_expression.csv", help='Location of CSV with differential expression results under --indir (default: analysis/diffexp/gene_expression_graphclust/differential_expression.csv)')

    aux_conv_params = parser.add_argument_group("Auxiliary PMTiles Conversion Parameters")
    aux_conv_params.add_argument('--min-zoom', type=int, default=10, help='Minimum zoom level (default: 10)')
    aux_conv_params.add_argument('--max-zoom', type=int, default=18, help='Maximum zoom level (default: 18)')
    aux_conv_params.add_argument('--max-tile-bytes', type=int, default=5000000, help='Maximum bytes for each tile in PMTiles (default: 5000000)')
    aux_conv_params.add_argument('--max-feature-counts', type=int, default=500000, help='Max feature limits per tile in PMTiles (default: 500000)')
    aux_conv_params.add_argument('--preserve-point-density-thres', type=int, default=1024, help='Threshold for preserving point density in PMTiles (default: 1024)')

    aux_params = parser.add_argument_group("Auxiliary Parameters", "Advanced settings; defaults work for most cases")    
    aux_params.add_argument('--tsv-cmap', type=str, default=f"{repo_dir}/assets/fixed_color_map_60.tsv", help=f'Location of TSV with color mappings for clusters (default: {repo_dir}/assets/fixed_color_map_60.tsv)')
    aux_params.add_argument('--de-max-pval', type=float, default=0.01, help='Maximum p-value for differential expression (default: 0.01)')
    aux_params.add_argument('--de-min-fc', type=float, default=1.2, help='Minimum fold change for differential expression (default: 1.2)')
    aux_params.add_argument('--col-rename', type=str, nargs='+', help='Columns to rename in the output file. Format: old_name1:new_name1 old_name2:new_name2 ...')
    aux_params.add_argument('--catalog-yaml', type=str, help='Path to catalog.yaml to update (used with --update-catalog; default: <indir>/catalog.yaml)')
    #aux_params.add_argument('--keep-intermediate-files', action='store_true', default=False, help='Keep intermediate output files')
    aux_params.add_argument('--tmp-dir', type=str, help='Temporary directory for intermediate files (default: out-dir/tmp or /tmp if specified)')

    env_params = parser.add_argument_group("Env Parameters", "Tool paths (override defaults if needed)")
    env_params.add_argument('--tippecanoe', type=str, default=f"{repo_dir}/submodules/tippecanoe/tippecanoe", help='Path to tippecanoe binary (default: <cartloader_dir>/submodules/tippecanoe/tippecanoe)')

    if len(_args) == 0:
        parser.print_help()
        sys.exit(1)

    args = parser.parse_args(_args)

    # Sanity check: JSON mode and manual mode are mutually exclusive
    if args.in_json and args.indir:
        parser.error("Cannot enable both JSON mode (--in-json) and manual input mode (--indir and/or --geojson-cells/--mtx-cell/--csv-clust/--csv-diffexp). Choose one.")

    return args

def import_visiumhd_cell(_args):
    """
    Import cell segmentation results from Space Ranger output
    """

    # parse argument
    args=parse_arguments(_args)

    logger = create_custom_logger(__name__, args.outprefix + "_importxenium" + args.log_suffix if args.log else None)
    logger.info("Analysis Started")

    if args.all:
        args.cells = True
        args.boundaries = True
        args.summary = True
        # args.update_catalog = True
    
    if not args.cells and not args.boundaries and not args.summary and not args.update_catalog:
        raise ValueError("At least one action should be activated.")

    # create output directory if needed
    out_dir = os.path.dirname(args.outprefix)
    out_base = os.path.basename(args.outprefix)
    if not os.path.exists(out_dir) and out_dir != "":
        os.makedirs(out_dir, exist_ok=True)

    if args.tmp_dir is None:
        args.tmp_dir = os.path.join(out_dir, "tmp")
        if not os.path.exists(args.tmp_dir):
            os.makedirs(args.tmp_dir, exist_ok=True)

    #temp_fs=[]

    # read in_json if provided 
    if args.in_json is not None:
        assert os.path.exists(args.in_json), f"File not found: {args.in_json} (--in-json)"
        raw_data=load_file_to_dict(args.in_json)
        cell_data=raw_data.get("CELLS", raw_data) # # use raw_data as default to support the flat dict build in the old scripts
    else:
        cell_data={
            "CELL_FEATURE_MEX": f"{args.indir}/{args.mtx_cell}",
            "CELL_GEOJSON": f"{args.indir}/{args.csv_boundaries}",
            "CLUSTER": f"{args.indir}/{args.csv_clust}",
            "DE": f"{args.indir}/{args.csv_diffexp}",
        }

    # Cluster/DE
    if args.cells or args.boundaries:
        clust_in = cell_data.get("CLUSTER", None)
        assert clust_in is not None, ('Path not provided: "CLUSTER" in --in-json' if args.in_json is not None else 'Path not provided: --csv-clust')
        assert os.path.exists(clust_in), (f'File not found: {clust_in} ("CLUSTER" in --in-json)' if args.in_json is not None else f'File not found: {clust_in} (--csv-clust)')

        logger.info(f"Loading cell cluster data from {clust_in}")
        sorted_clusters, cluster2idx, bcd2cluster, bcd2clusteridx=process_cluster_csv(clust_in)
        logger.info(f"  * Loaded {len(bcd2cluster)} cells")
        
        ## read/write DE results
        de_in = cell_data.get("DE", None)
        assert de_in is not None, ('Path not provided: "DE" in --in-json' if args.in_json is not None else 'Path not provided: --csv-diffexp')
        assert os.path.exists(de_in), (f'File not found: {de_in} ("DE" in --in-json)' if args.in_json is not None else f'File not found: {de_in} (--csv-diffexp)')        

        logger.info(f"  * Reading DE results from {de_in}")
        clust2genes=read_de_csv(de_in, cluster2idx, args.de_min_fc, args.de_max_pval)

        ## write DE results
        de_out=f"{args.outprefix}-cells-bulk-de.tsv"
        logger.info(f"  * Writing DE results for {len(clust2genes)} clusters) to {de_out}")
        write_de_tsv(clust2genes, de_out, sorted_clusters)

        ## write the color map
        assert os.path.exists(args.tsv_cmap), f"File not found: {args.tsv_cmap} (--tsv-cmap)"        

        cmap_out=f"{args.outprefix}-rgb.tsv"
        logger.info(f"  * Writing color map from {args.tsv_cmap} to {cmap_out}")
        write_cmap_tsv(cmap_out, args.tsv_cmap, sorted_clusters)

    # Process segmented calls 
    if args.cells or args.boundaries:
        cells_json=cell_data.get("CELL_GEOJSON", None)
        assert os.path.exists(cells_json), (f'Path not provided: CELL_GEOJSON in --in-json' if args.in_json is not None else f'Path not provided: --geojson-cells')
        assert os.path.exists(cells_json), (f'File not found: {cells_json} (CELL_GEOJSON in --in-json)' if args.in_json is not None else f'File not found: {cells_json} (--geojson-cells)')
    
    if args.cells:
        cell_ftr_mex = cell_data.get("CELL_FEATURE_MEX", None)
        assert cell_ftr_mex is not None, ('Path not provided: CELL_FEATURE_MEX in --in-json' if args.in_json is not None else 'Path not provided: --mtx-cell')
        assert os.path.isdir(cell_ftr_mex), (f'Directory not found: {cell_ftr_mex} (CELL_FEATURE_MEX in --in-json)' if args.in_json is not None else f'Directory not found: {cell_ftr_mex} (--mtx-cell)')
        # required files exist
        bcd_path = os.path.join(cell_ftr_mex, "barcodes.tsv.gz")
        mtx_path = os.path.join(cell_ftr_mex, "matrix.mtx.gz")
        assert os.path.exists(bcd_path), (f"File not found: {bcd_path} (barcodes.tsv.gz in CELL_FEATURE_MEX from --in-json)" if args.in_json is not None else f"File not found: {bcd_path} (barcodes.tsv.gz from --mtx-cell)")
        assert os.path.exists(bcd_path), (f"File not found: {mtx_path} (matrix.mtx.gz in CELL_FEATURE_MEX from --in-json)" if args.in_json is not None else f"File not found: {mtx_path} (matrix.mtx.gz from --mtx-cell)")

        logger.info(f"Processing cell information from {cells_json}; {cell_ftr_mex}")

        # convert cell geojson into csv format with cell_id, x, y, count
        cells_out=f"{args.outprefix}-cells.csv"
        logger.info(f"  * Reading cell data from {cells_json}; {cell_ftr_mex} and extracting geometry to {cells_out}")
        process_cell_geojson_w_mtx(cells_json, cell_ftr_mex, cells_out, bcd2clusteridx)

        cells_pmtiles = f"{args.outprefix}-cells.pmtiles"
        logger.info(f"  * Generating PMTiles from cell geometry data into cell pmtiles: {cells_pmtiles}")
        tile_csv_into_pmtiles(cells_out, cells_pmtiles, args, logger, no_dup=True)
        #temp_fs.append(cells_out)

    # Process cell boundaries
    if args.boundaries:
        logger.info(f"Processing cell boundary information from {cells_json}")
        ## read the cell CSV files
        bound_out=f"{args.outprefix}-boundaries.geojson"
        logger.info(f"  * Reading cell boundary data from {cells_json} and extracting geometry to {bound_out}")
        process_boundaries_geojson(cells_json, bound_out, bcd2clusteridx)

        ## Run the tippecanoe command
        bound_pmtiles = f"{args.outprefix}-boundaries.pmtiles"
        logger.info(f"  * Generating PMTiles from boundary geometry data into boundary pmtiles: {bound_pmtiles}")
        tile_csv_into_pmtiles(bound_out, bound_pmtiles, args, logger, no_dup=False)
        #temp_fs.append(bound_out)
    
    # JSON/YAML
    if args.summary or args.update_catalog:
        ## add the new factor to the catalog
        factor_id = out_base if args.id is None else args.id
        factor_name = out_base if args.name is None else args.name
        pmtiles_keys=[]
        if os.path.exists(f"{args.outprefix}-cells.pmtiles"):
            pmtiles_keys.append("cells")
        if os.path.exists(f"{args.outprefix}-boundaries.pmtiles"):
            pmtiles_keys.append("boundaries")

    if args.summary:
        out_assets_f=f"{args.outprefix}_assets.json"
        logger.info(f"Summarizing assets information into {out_assets_f}")
        new_factor = make_factor_dict(factor_id, factor_name, args.outprefix, pmtiles_keys)
        write_dict_to_file(new_factor, out_assets_f, check_equal=True)

    if args.update_catalog:
        ## read the input catalog.yaml file
        if args.catalog_yaml is None:
            args.catalog_yaml = os.path.join(args.indir, "catalog.yaml")
        
        logger.info(f"Updating catalog YAML file: {args.catalog_yaml}")

        ## load the YAML file
        with open(args.catalog_yaml, 'r') as f:
            catalog = yaml.load(f, Loader=yaml.FullLoader)  # Preserves order

        ## add files to the catalog
        new_factor = make_factor_dict(factor_id, factor_name, out_base, pmtiles_keys)
        if "factors" not in catalog["assets"]:
            raise ValueError("No factors found in the catalog.yaml file. Check if the file is correct.")
        catalog["assets"]["factors"].append(new_factor)

        ## write the updated catalog.yaml file
        ##TO-DO: 
        # - catalog_yaml should be within the same dir with the output assets
        # - could update catalog directly without creating a new catalog file

        out_yaml = f"{args.outprefix}-catalog.yaml"
        with open(out_yaml, 'w') as f:
            yaml.dump(catalog, f, Dumper=yaml.SafeDumper, default_flow_style=False, sort_keys=False)
        logger.info(f"Successfully wrote the catalog.yaml file: {out_yaml}")

    ## clean the temp files
    # if not args.keep_intermediate_files:
    #     logger.info(f"Cleaning intermediate files")
    #     if len(temp_fs) >0:
    #         for temp_f in temp_fs:
    #             if os.path.exists(temp_f):
    #                 os.remove(temp_f)


    logger.info("Analysis Finished")

if __name__ == "__main__":
    # Get the base file name without extension
    script_name = os.path.splitext(os.path.basename(__file__))[0]

    # Dynamically get the function based on the script name
    func = getattr(sys.modules[__name__], script_name)

    # Call the function with command line arguments
    func(sys.argv[1:])
