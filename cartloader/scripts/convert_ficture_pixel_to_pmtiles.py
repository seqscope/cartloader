import sys, os, re, gzip, logging, argparse, inspect, subprocess, gzip
import pandas as pd
import numpy as np

from cartloader.utils.utils import create_custom_logger

def convert_ficture_pixel_to_pmtiles(_args):
    """
    Convert the pixel-level decoding output from FICTURE to a TSV format that can be converted to pmtiles
    """
    repo_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))

    parser = argparse.ArgumentParser(prog=f"cartloader {inspect.getframeinfo(inspect.currentframe()).function}", description="Convert the pixel-level decoding output from FICTURE to a TSV format that can be converted to pmtiles")
    inout_params = parser.add_argument_group("Input/Output Parameters", "Input/output directory/files.")
    inout_params.add_argument('--in-pixel', type=str, required=True, help='Pixel-level output generated by FICTURE. Typically named as ....pixel.sorted.tsv.gz')
    inout_params.add_argument('--out-prefix', type=str, required=True, help='Prefix of output files. New directory will be created if needed')

    iocol_params = parser.add_argument_group("Input/Output Columns Parameters", "Input/output column parameters .")
    iocol_params.add_argument('--max-k', type=int, default=1, help='Maximum number of factors to output as column K1, K2, K3... Default is 1')
    iocol_params.add_argument('--output-prob', action='store_true', default=False, help='Output the probability of each factor as P1, P2, P3...')
    iocol_params.add_argument('--colname-in-x', type=str, default='X', help='Name of X coordinate of spatial position in the input TSV file')
    iocol_params.add_argument('--colname-in-y', type=str, default='Y', help='Name of Y coordinate of spatial position in the input TSV file')
    iocol_params.add_argument('--colname-out-x', type=str, default='lon', help='Name of X coordinate of spatial position in the output CSV/TSV file')
    iocol_params.add_argument('--colname-out-y', type=str, default='lat', help='Name of Y coordinate of spatial position in the output CSV/TSV file')
    iocol_params.add_argument('--colname-factor-prefix', type=str, default='K', help='Prefix of factor columns (default: K)')
    iocol_params.add_argument('--colname-prob-prefix', type=str, default='P', help='Prefix of probability columns (default: P)')
    iocol_params.add_argument('--meta-offset-x', type=str, default='OFFSET_X', help='Keyword for X offset in the metadata')
    iocol_params.add_argument('--meta-offset-y', type=str, default='OFFSET_Y', help='Keyword for Y offset in the metadata')
    iocol_params.add_argument('--meta-scale', type=str, default='SCALE', help='Keyword for scale in the metadata')

    aux_params = parser.add_argument_group("Auxiliary Parameters", "Auxiliary parameters frequently used by users")
    aux_params.add_argument('--log', action='store_true', default=False, help='Write log to file')
    aux_params.add_argument('--skip-pmtiles', action='store_true', default=False, help='Keep intermediate output files')
    aux_params.add_argument('--keep-intermediate-files', action='store_true', default=False, help='Keep intermediate output files')
    aux_params.add_argument('--chunk-size', type=int, default=1000000, help='Number of rows to read at a time. Default is 1000000')
    aux_params.add_argument('--log-suffix', type=str, default=".log", help='The suffix for the log file (appended to the output directory). Default: .log')
    aux_params.add_argument('--in-delim', type=str, default='\t', help="Delimiter of the input file")
    aux_params.add_argument('--out-delim', type=str, default=',', help="Delimiter of the output file")
    aux_params.add_argument('--out-csv-suffix', type=str, default='.csv', help="Suffix of output CSV file")
    aux_params.add_argument('--out-pmtiles-suffix', type=str, default='.pmtiles', help="Suffix of output PMTiles file")
    aux_params.add_argument('--tippecanoe', type=str, default=f"{repo_dir}/submodules/tippecanoe/tippecanoe", help='Path to tippecanoe binary')
    aux_params.add_argument('--min-zoom', type=int, default=10, help='Minimum zoom level')
    aux_params.add_argument('--max-zoom', type=int, default=18, help='Maximum zoom level')
    aux_params.add_argument('--max-tile-bytes', type=int, default=1500000, help='Maximum bytes for each tile')
    aux_params.add_argument('--preserve-point-density-thres', type=int, default=64, help='Threshold for preserving point density')

    args = parser.parse_args(_args)

    logger = create_custom_logger(__name__, args.out_prefix + "_convert_ficture_pixel_to_pmtiles" + args.log_suffix if args.log else None)

    logger.info("Reading the metadata")

    ## parse the header information
    meta_dict = {}
    header = None
    with gzip.open(args.in_pixel, 'rt', encoding='utf-8') as rf:
        for line in rf:
            if line.startswith("##"): ## meta lines
                meta_line = line[2:]
                for meta_keyval in meta_line.rstrip().split(';'):
                    key, val = meta_keyval.split('=')
                    meta_dict[key] = val
            elif line.startswith("#"): ## header line
                header = line[1:].rstrip().split('\t')
                break

    ## ensure that all the necessary info is available
    try:
        offset_x = float(meta_dict[args.meta_offset_x])
        offset_y = float(meta_dict[args.meta_offset_y])
        scale = float(meta_dict[args.meta_scale])
    except KeyError as e:
        logger.error(f"Missing key: {e} in the metadata of the input file {args.in_pixel}")
        sys.exit(1)

    ## determine header columns to extract
    index_cols = []
    name_cols = []
    try:
        index_cols.append(header.index(args.colname_in_x))
        name_cols.append(args.colname_out_x)
        index_cols.append(header.index(args.colname_in_y))
        name_cols.append(args.colname_out_y)
        for i in range(args.max_k):
            colname = args.colname_factor_prefix + str(i+1)
            name_cols.append(colname)
            index_cols.append(header.index(colname))
        if args.output_prob:
            for i in range(args.max_k):
                colname = args.colname_prob_prefix + str(i+1)
                name_cols.append(colname)
                index_cols.append(header.index(colname))
    except ValueError as e:
        logger.error(f"Missing required column names in the input file {args.in_pixel}\nCurrent header: {header}")
        sys.exit(1)

    logger.info("Converting the pixel-level decoding data to CSV/TSV format")

    n_chunks = 0
    for chunk in pd.read_csv(args.in_pixel, sep=args.in_delim, chunksize=args.chunk_size, usecols=index_cols, header=None, comment='#'):
        chunk.columns = name_cols

        ## add offset and scale X/Y
        chunk[args.colname_out_x] = chunk[args.colname_out_x] / scale + offset_x
        chunk[args.colname_out_y] = chunk[args.colname_out_y] / scale + offset_y

        ## write to a CSV file
        if n_chunks == 0:
            chunk.to_csv(f"{args.out_prefix}{args.out_csv_suffix}", sep=args.out_delim, index=False, header=True)
        else:
            chunk.to_csv(f"{args.out_prefix}{args.out_csv_suffix}", sep=args.out_delim, index=False, header=False, mode='a')

        n_chunks += 1
        logger.info(f"Finished processing chunk {n_chunks} of size {args.chunk_size}...")

    ## Converting the output to pmtiles
    if not args.skip_pmtiles:
        logger.info("Converting the CSV file to PMTiles format")
        cmd = f"{args.tippecanoe} -o {args.out_prefix}{args.out_pmtiles_suffix} -Z {args.min_zoom} -z {args.max_zoom} --force -s EPSG:3857 -M {args.max_tile_bytes} --drop-densest-as-needed --extend-zooms-if-still-dropping '--preserve-point-density-threshold={args.preserve_point_density_thres}' --no-duplication {args.out_prefix}{args.out_csv_suffix}"
        print("Command to run:" + cmd)
        result = subprocess.run(cmd, shell=True)
        if result.returncode != 0:
            logger.error("Error in converting the input TSV file into output CSV/TSV file")
            sys.exit(1)

    ## Remove the intermediate CSV file
    if not args.keep_intermediate_files:
        logger.info("Removing the intermediate CSV file {args.out_prefix}{args.out_csv_suffix}...")
        os.remove(f"{args.out_prefix}{args.out_csv_suffix}")

    logger.info("Analysis Finished")

if __name__ == "__main__":
    # Get the base file name without extension
    script_name = os.path.splitext(os.path.basename(__file__))[0]

    # Dynamically get the function based on the script name
    func = getattr(sys.modules[__name__], script_name)

    # Call the function with command line arguments
    func(sys.argv[1:])