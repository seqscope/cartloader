import os
import subprocess
import argparse
import sys
import yaml
from typing import Dict, List, Tuple

from cartloader.utils.minimake import minimake
from cartloader.utils.utils import cmd_separator, scheck_app, create_custom_logger

def run_command(command, dry_run):
    if dry_run:
        print("Dry-run command:", " ".join(command))
    else:
        subprocess.run(command, check=True)

def traverse_dict(d, parent_key=''):
    pairs = []
    for key, value in d.items():
        new_key = f"{parent_key}.{key}" if parent_key else key
        if isinstance(value, dict):
            pairs.extend(traverse_dict(value, new_key))  # Recurse into dictionaries and extend pairs list
        elif isinstance(value, list):
            for i, item in enumerate(value):
                pairs.extend(traverse_dict({f"item_{i}": item}, new_key))  # Recurse into list items
        else:
            pairs.append((new_key, value))  # Append key-value pair
    return pairs

def collect_files_from_yaml(catalog_f):
    # Option 1: use "." to locate the files
    # with open(catalog_f, "r") as catalog_file:
    #     for line in catalog_file:
    #         if "." in line:
    #             # Extract the file name
    #             file_name = line.strip().split()[-1]
    #             file_path = os.path.join(args.in_dir, file_name)
    #             s3_file_path = f"{args.s3_dir}/{file_name}"
    #             commands.append(f"echo 'Uploading {file_name} to S3...'")
    #             commands.append(["aws", "s3", "cp", file_path, s3_file_path])
    # Option 2: use traverse_dict to locate the files. (function: collect_files_from_yaml)
    # Note, if catalog.yaml file structure changes, this keys_id may need to be updated.
    print(f"Extracting input files from catalog file: {catalog_f}")
    with open(catalog_f, "r") as catalog_file:
        catalog = yaml.safe_load(catalog_file)
    catalog_pair=traverse_dict(catalog)

    keys_id = {"id", "title", "name", "model_id", "proj_id", "decode_id"}
    cartload_files = []
    hist_files = []

    for key, value in catalog_pair:
        if key.startswith("assets.basemap"):
            if key.startswith("assets.basemap.sge") and not key.endswith("default"):
                cartload_files.append(value)
            elif not key.startswith("assets.basemap.sge") and not key.endswith("default"):
                hist_files.append(value)
        else:
            subkey = key.split(".")[-1] if "." in key else key
            if subkey not in keys_id:
                cartload_files.append(value)

    # Deduplicate cartload_files
    cartload_files = list(set(cartload_files))

    return cartload_files, hist_files

def upload_aws(_args):
    parser = argparse.ArgumentParser(description="Upload files to S3 as specified in catalog.yaml. All files must be in the input directory.")
    run_params = parser.add_argument_group("Run Options", "")
    run_params.add_argument('--dry-run', action='store_true', default=False, help='Dry run. Generate only the Makefile without running it')
    run_params.add_argument('--restart', action='store_true', default=False, help='Restart the run. Ignore all intermediate files and start from the beginning')
    run_params.add_argument('--n-jobs', type=int, default=1, help='Number of jobs (processes) to run in parallel')
    run_params.add_argument('--makefn', type=str, default="upload_aws.mk", help='The name of the Makefile to generate (default: upload_aws.mk)')

    key_params = parser.add_argument_group("Key Parameters", "Key parameters that requires user's attention")
    key_params.add_argument("--in-dir", help="Path to the input directory (e.g., /<main_dir>/cartload/).")
    key_params.add_argument("--s3-dir", help="S3 directory path (e.g., s3://<bucket_name>/<ID>).")
    key_params.add_argument("--catalog-yaml", default=None, help="Path to the catalog.yaml file (default: /<in_dir>/catalog.yaml).")
    key_params.add_argument("--upload-cartload-only", action="store_true", default=False, help="Only upload the files generated by run_cartload*")
    key_params.add_argument("--upload-histology-only", action="store_true", default=False, help="Only upload the additional histology pmtiles")
    key_params.add_argument('--aws', type=str, default="aws", help='The path to aws (default: aws)')
    args = parser.parse_args(_args)

    # catalog files
    if args.catalog_yaml is None:
        catalog_f= os.path.join(args.in_dir, "catalog.yaml")
    else:
        catalog_f = args.catalog_yaml
    
    s3_catalog_f = f"{args.s3_dir}/catalog.yaml"
    assert os.path.exists(catalog_f), "Provide an invalid catalog yaml file"

    # get the cartload output and basemaps files from catalog
    cartload_files, hist_files = collect_files_from_yaml(catalog_f)

    # start mm
    mm = minimake()
        
    # step 1. Upload cartload files to AWS ï¼ˆall files, except the nonsge in basemaps 
    cmds=cmd_separator([], f"Uploading cartload files to AWS...")
    if not args.upload_histology_only:
        cartload_prerequisites=[os.path.join(args.in_dir, filename) for filename in cartload_files]
        cartload_prerequisites.append(catalog_f)

        for filename in cartload_files:
            file_path=os.path.join(args.in_dir, filename)
            s3_file_path = os.path.join(args.s3_dir, filename) 
            cmds.append(f"{args.aws} s3 cp {file_path} {s3_file_path}")
        
        cmds.append(f"{args.aws} s3 cp  {catalog_f} {s3_catalog_f}")

        cartload_flag=os.path.join(args.in_dir, "cartload.aws.done")
        cmds.append(f"touch {cartload_flag}")

        mm.add_target(cartload_flag, cartload_prerequisites, cmds)

    # step 2. Upload basemap files to AWS besides cartload
    if not args.upload_cartload_only:
        for filename in hist_files:
            cmds=cmd_separator([], f"Uploading additional basemaps to AWS: {filename}...")
            file_path=os.path.join(args.in_dir, filename)
            s3_file_path = os.path.join(args.s3_dir, filename) 
            cmds.append(f'{args.aws} s3 cp "{file_path}" "{s3_file_path}" && {args.aws} s3 cp "{catalog_f}" "{s3_catalog_f}" && touch {file_path}.aws.done')
            mm.add_target(f"{file_path}.aws.done", [file_path], cmds)

    ## write makefile
    make_f = os.path.join(args.in_dir, args.makefn)
    mm.write_makefile(make_f)

    if args.dry_run:
        dry_cmd=f"make -f {make_f} -n {'-B' if args.restart else ''} "
        os.system(dry_cmd)
        print(f"To execute the pipeline, run the following command:\nmake -f {make_f} -j {args.n_jobs}")
    else:
        exe_cmd=f"make -f {make_f} -j {args.n_jobs} {'-B' if args.restart else ''}"
        result = subprocess.run(exe_cmd, shell=True)
        if result.returncode != 0:
            print(f"Error in executing: {exe_cmd}")
            sys.exit(1)

if __name__ == "__main__":
    # get the cartloader path
    global cartloader_repo
    cartloader_repo=os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    
    # Get the base file name without extension
    script_name = os.path.splitext(os.path.basename(__file__))[0]

    # Dynamically get the function based on the script name
    func = getattr(sys.modules[__name__], script_name)

    # Call the function with command line arguments
    func(sys.argv[1:])